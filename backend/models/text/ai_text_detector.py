from __future__ import annotations

import os
from typing import Any, Dict

import torch
import yaml
from transformers import AutoModelForSequenceClassification, AutoTokenizer

from utils.logger import logger

CONFIG_PATH = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
    "config", "config.yaml",
)


def _load_config() -> dict:
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


class TextAIDetector:
    """Detects whether a piece of text was generated by an AI model."""

    def __init__(self) -> None:
        cfg = _load_config()
        text_cfg = cfg["text"]
        self.model_name: str = text_cfg["model_name"]
        self.max_length: int = text_cfg.get("max_length", 512)
        self.device = torch.device(
            cfg.get("device", "cuda") if torch.cuda.is_available() else "cpu"
        )

        logger.info("Loading text model '%s' on %s", self.model_name, self.device)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)
        self.model.to(self.device)
        self.model.eval()
        logger.info("Text model loaded successfully")

    @torch.no_grad()
    def predict(self, text: str) -> Dict[str, Any]:
        """Classify *text* as human-written or AI-generated.

        Returns a dict with:
            - label      : "ai-generated" or "human-written"
            - confidence : probability of the predicted label
            - ai_probability   : probability the text is AI-generated
            - human_probability: probability the text is human-written
        """
        if not text or not text.strip():
            logger.warning("Empty text provided for AI-text detection")
            return {
                "label": "unknown",
                "confidence": 0.0,
                "ai_probability": 0.0,
                "human_probability": 0.0,
            }

        logger.info("Tokenising input text (length=%d chars, max_length=%d)", len(text), self.max_length)
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=self.max_length,
            padding=True,
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        logits = self.model(**inputs).logits
        probs = torch.softmax(logits, dim=-1).cpu().squeeze().tolist()

        id2label = self.model.config.id2label

        fake_idx = None
        real_idx = None
        for idx, lbl in id2label.items():
            lower = str(lbl).lower()
            if lower in {"fake", "ai", "ai-generated", "machine"}:
                fake_idx = int(idx)
            elif lower in {"real", "human", "human-written"}:
                real_idx = int(idx)

        if fake_idx is None or real_idx is None:
            fake_idx, real_idx = 0, 1
            logger.debug(
                "Could not map labels automatically; assuming idx 0=fake, 1=real (labels: %s)",
                id2label,
            )

        ai_prob = round(probs[fake_idx], 4)
        human_prob = round(probs[real_idx], 4)

        label = "ai-generated" if ai_prob >= human_prob else "human-written"
        confidence = max(ai_prob, human_prob)

        result: Dict[str, Any] = {
            "label": label,
            "confidence": confidence,
            "ai_probability": ai_prob,
            "human_probability": human_prob,
        }

        logger.info("Text result: label=%s, confidence=%.4f", label, confidence)
        return result
